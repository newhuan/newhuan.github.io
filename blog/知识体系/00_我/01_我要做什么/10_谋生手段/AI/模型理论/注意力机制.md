在一篇议论文中，作者可能在文章开头提出一个论点，然后通过一系列的论据和分析来支持这个论点，直到文章结尾可能再次强调或总结这个论点。
——
不过虽然词元与各个词元之间都可能存在依赖关系，但是其依赖程度不同，为了体现词元之间的依赖程度，NLP选择引入“注意力机制”。“注意力机制”可以动态地捕捉序列中不同位置元素之间的依赖关系，分析其强弱程度，并根据这些依赖关系生成新的序列表示。其核心思想是模仿人类的注意力，即在处理大量信息时，能够聚焦于输入数据的特定部分，忽略掉那些不太重要的信息，从而更好地理解输入内容。

继续以“我配拥有一杯咖啡吗？”为例，读到“拥有”这个词元时，我们会发现“我”是“拥有”的主语，“配”是对“拥有”的强调，他们都与“拥有”产生了依赖关系。这句话的核心思想，是某人认为自己有资格拥有某物，所以可能“配”相对“我”而言，对“拥有”来说更重要，那么我们在分析“拥有”这个词的语义时，会给“配”分配更多的注意力，这通常体现为分配更高的“注意力权重”。﻿

实际上，我们在日常工作中已经应用了注意力机制。比如，我们都知道思维链（COT，常用<输入, 思考, 输出>来描述）对大模型处理复杂问题时很有帮助，其本质就是将复杂的问题拆分成相对简单的小问题并分步骤处理，使模型能够聚焦于问题的特定部分，来提高输出质量和准确性。“聚焦”的这一过程，就是依赖模型的注意力机制完成。通常模型会依赖输出内容或内部推理（如o1具有内部推理过程，即慢思考）来构建思考过程，但哪怕没有这些内容，仅仅依靠注意力本身，COT也能让模型提高部分性能。
——
时间来到2018年，OpenAI团队的论文《Improving Language Understanding by Generative Pre-Training》横空出世，它提出可以在大规模未标注数据集上预训练一个通用的语言模型，再在特定NLP子任务上进行微调，从而将大模型的语言表征能力迁移至特定子任务中。其创新之处在于，提出了一种新的预训练-微调框架，并且特别强调了生成式预训练在语言模型中的应用。生成式，指的是通过模拟训练数据的统计特性来创造原始数据集中不存在的新样本，这使得GPT在文本生成方面具有显著的优势。