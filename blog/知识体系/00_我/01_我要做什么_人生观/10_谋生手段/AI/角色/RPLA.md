RPLA（角色扮演语言智体）
https://blog.csdn.net/yorkhunter/article/details/138780138
From Persona to Personalization: A Survey on Role-Playing Language Agents

为LLM配备必要的类人能力，例如规划、工具使用和记忆

规划模块：CoT\ReAct
工具使用模块：API、知识库、外部模型、针对特定应用的定制操作
记忆机制：感觉记忆、短期记忆、长期记忆；对话上下文、向量库、自然语言数据库

人口统计角色：职业、爱好或兴趣、性格类型；语言风格、专业知识、行为差别
人物角色：历史名人、虚拟角色
个性化定制角色：个人的数字克隆或个人助理


- 对话：个性化 RPLA 的早期研究主要集中在通过学习和整合用户角色来实现个性化对话（Cho，2022；Zhou，2023c；Ng，2024），将风格特征与用户角色相结合。 用户偏好以提高参与度（Zheng，2021；Wang）。随着法学硕士的出现和发展，个性化的RPLA变得能够处理日益复杂和全面的任务，获得复杂任务规划和工具学习的能力，以自动完成个性化服务。
- 推荐：基于LLM的会话推荐系统（Chen et al., 2023b; Yang et al., 2023a; Wu et al., 2023）已被广泛认为是下一代推荐系统（Lian et al., 2024），通过多轮对话支持用户实现推荐相关目标（Jannach et al., 2021）。与传统的推荐相比，这些方法以其坚实的基础模型、自然语言交互以及简单、典型的非参数进化而脱颖而出（Sallam，2023；Abbasian，2023）。
- 任务解决：此外，个性化 RPLA 在解决更复杂的任务方面变得越来越有能力（Yao，2023a；Significant-Gravitas，2023），例如编码（Microsoft，2024）、旅行计划（Xie，2024a） ）和研究调查（Wang，2024b），通常与各种外部软件交互。 它们是基于 LLM的自主智体，与个人数据、设备和服务深度集成（Dong，2023；Li，2024d）。 他们拥有比 Siri（Apple，2024）等早期更先进的个人助理，后者难以应对复杂的用户请求。


为了构建准确捕捉和描绘个性化角色的个性化 RPLA，该过程通常包括两个关键步骤：1) 角色数据收集，收集必要的数据来塑造个性化角色；2) 角色建模，这些个人角色使用收集的数据创建表征模型。 对于角色数据收集，不同应用程序和任务中的数据在格式、内容和模式上可能存在很大差异。 这些数据分为三种类型：个人资料、交互和领域知识。 对于角色建模，挑战是从未处理的角色数据中体现预期的角色，这些数据通常是大量、稀疏和嘈杂的。

对“人”建模；人的信息收集/感受、人的判断、人的反馈、人的决策、人的行动 => 影响因素很多
道德、法律、政治

未来的研究方向
- 用于决策的因果数据分析：角色扮演决策必须出于合理的原因而做出，这需要模型超越对可观察行为的简单模仿，包括对其潜在因果关系的理解。 从相互交织的经验中提取和确认因果因素的复杂性带来了重大挑战，需要先进的分析和更深入的数据解释策略，以使 RPLA 能够做出明智的决策。
- 改进决策：决策过程不仅仅是复制历史，而是量身定制以确保个别情况的最佳结果。 这包括表现出先进（如果不是超人）智慧的决策、避免错误或在艰难的困境中做出最佳选择。 此类机构要求 RPLA 和底层LLM能够全面收集和利用与其角色相关的背景和复杂性。
- RPLA 作为个人决策的个人助理：RPLA 未来发展为综合个人助理标志着重大转变。 这些系统可以管理互联网行为的各个方面，从定制购物和个性化旅行计划到新一代推荐系统。 通过整合图像和视频等多模态数据处理，并与先进的可视化技术相结合，RPLA 可以显着提高日常任务的个性化和效率。
- 通过自主角色扮演进行社会模拟：利用 RPLA 进行社会模拟可以在不同场景中进行精细实验研究心理和社会行为，从而显着扩展其应用。 通过扮演各种角色，RPLA 可以作为多功能测试目标，探索不同人格特质对社交智力的影响，为人类行为和互动动态提供有价值的见解。

——
我想先说一下我认为之前的方式的问题，即人和 AI 角色直接聊天的那种交互方式，这是一种最自然，最容易想到的交互，但在现有的技术下，它有一个最致命的问题，就是使用越多，效果越差。用户聊的越多，投入的情感越多，积累的信息也越多，上下文也会更多，因此成本就越高，而模型也一定会变得更笨，虽然通过各种类 RAG 的方式可以解决部分「记忆」问题，但这都不是本质上去解决问题，这个感觉挺让人难过的，它甚至有一点反网络效应——新用户可能体验还好，重度用户可能被迫要接受越来越差的效果。
